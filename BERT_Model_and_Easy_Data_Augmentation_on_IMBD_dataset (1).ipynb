{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Model and Easy Data Augmentation on IMBD dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhTien2xzMN7"
      },
      "source": [
        "# Introduction\n",
        "In this project we will look at the IMDB movie reviews data set and apply an Easy Data Augmentation to see if it improves the accuracy from the sentiment analysis. The work is unpolished, in the sense that there are more augmentation techniques that can be used, but this project focuses on using swap augmentation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNXgRRFWRZ1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db16cb71-05d6-410a-91b7-14bfcfc796ae"
      },
      "source": [
        "!pip install -q tensorflow-text\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vPvfQHeRlcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ddb2b4-a99a-4d56-8aa9-4f06a34b3900"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 25.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 19.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 46.2 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccoiiaqORnzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecf04e3-5f31-451b-be44-5e31a636a0b7"
      },
      "source": [
        "try:\n",
        "  import textaugment\n",
        "except ModuleNotFoundError:\n",
        "  !pip install textaugment\n",
        "  import textaugment\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textaugment\n",
            "  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from textaugment) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textaugment) (3.2.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from textaugment) (0.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from textaugment) (1.19.5)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->textaugment) (5.2.1)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2021.10.8)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.11.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.7 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15734 sha256=338ed1df6018918899042f0acccd480e1e916326f9085aed92a49479e3b44e01\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/da/eb/a54579056f265eede0417df537dd56d3df5b9eb2b25df0003d\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans, textaugment\n",
            "Successfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0 textaugment-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPdJNPXTRtcK"
      },
      "source": [
        "# set parameters:\n",
        "max_features = 5000\n",
        "batch_size = 32\n",
        "epochs = 5"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWKerubR0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97ebe76-2bbe-41d4-c16d-3361ef8283fa"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "17473536/17464789 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMN6l8VjR5jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32d0352-d904-4653-8fb6-b1024af37879"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5Ruj3tdSV55"
      },
      "source": [
        "def decoding(dataset):\n",
        "  decoded_dataset = []\n",
        "  for text in dataset:\n",
        "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "    decoded_dataset.append(decoded_review)\n",
        "  return decoded_dataset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXVnX6wCSB8R"
      },
      "source": [
        "x_train_unprocessed = decoding(x_train)\n",
        "x_test_unprocessed = decoding(x_test)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtXp9c-QSl62"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train_unprocessed, y_train)).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test_unprocessed, y_test)).batch(batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-hutXRcUde2"
      },
      "source": [
        "val_ds = train_ds.take(160)\n",
        "real_train_ds = train_ds.skip(160)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cwmZF8EYS7p"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "real_train_ds = real_train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BjXBmDyYv2l"
      },
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyxZuN5MY5Il"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRNJTvLNY9HA"
      },
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzBheLRfZ6WI"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDCl5or1aBJF"
      },
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bChSvT_eaFmG"
      },
      "source": [
        "#Setting up the loss function\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUQjYAIaJdp"
      },
      "source": [
        "# Setting up the epochs and optimizer (AdamW optimizer) \n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOFWXUfXaOVO"
      },
      "source": [
        "# Setting up the classifier model with the optimizer, loss and metrics that we decided on\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvpbYJ7aUXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753d3180-0e5b-4f5e-e5b2-9a0d9a94b271"
      },
      "source": [
        "# Training the model\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=real_train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Epoch 1/5\n",
            "622/622 [==============================] - 290s 449ms/step - loss: 0.4913 - binary_accuracy: 0.7339 - val_loss: 0.3671 - val_binary_accuracy: 0.8385\n",
            "Epoch 2/5\n",
            "622/622 [==============================] - 279s 449ms/step - loss: 0.3432 - binary_accuracy: 0.8470 - val_loss: 0.3375 - val_binary_accuracy: 0.8492\n",
            "Epoch 3/5\n",
            "622/622 [==============================] - 280s 449ms/step - loss: 0.2714 - binary_accuracy: 0.8859 - val_loss: 0.3616 - val_binary_accuracy: 0.8504\n",
            "Epoch 4/5\n",
            "622/622 [==============================] - 279s 449ms/step - loss: 0.2127 - binary_accuracy: 0.9133 - val_loss: 0.3859 - val_binary_accuracy: 0.8602\n",
            "Epoch 5/5\n",
            "622/622 [==============================] - 278s 447ms/step - loss: 0.1684 - binary_accuracy: 0.9356 - val_loss: 0.4424 - val_binary_accuracy: 0.8621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIEbI-K9aZFr",
        "outputId": "943aba4d-7f1a-4799-eb4c-b62ba94c57ec"
      },
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 139s 178ms/step - loss: 0.4512 - binary_accuracy: 0.8636\n",
            "Loss: 0.4512398838996887\n",
            "Accuracy: 0.8636000156402588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRRTP-jMZY5z"
      },
      "source": [
        "We see that we get a baseline accuracy of 86%.  Now we will do the Easy Data Augmentation.  More specifically, we will apply swap augmentation, although it is easy to implement other augmentations as well, as they are still in the textaugment library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaak7hPeIpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cf3e0f-930e-4d6f-b2af-0b71e7229f34"
      },
      "source": [
        "from textaugment import EDA\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "t = EDA()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9MKZ54gte28"
      },
      "source": [
        "#does the swap augmentation on an already decoded dataset\n",
        "def swap_augment_ds(dataset):\n",
        "  swapped = []\n",
        "  for text in dataset:\n",
        "    swapped.append(t.random_swap(text))\n",
        "  return swapped\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk6ApkCWvJ6-"
      },
      "source": [
        "# Applying the swap augment twice because only once might not modify the training set enough\n",
        "swapped_x_train_unprocessed = swap_augment_ds(swap_augment_ds(decoding(x_train)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dogk3WaFvUpu"
      },
      "source": [
        "augmented_x_train = x_train_unprocessed + swapped_x_train_unprocessed\n",
        "augmented_y_train = np.concatenate((y_train , y_train))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpABxdjv6xu"
      },
      "source": [
        "aug_train_ds = aug_train_ds = tf.data.Dataset.from_tensor_slices((augmented_x_train,augmented_y_train)).batch(batch_size)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noYm8gmJwMLR"
      },
      "source": [
        "aug_val_ds = aug_train_ds.take(300)\n",
        "real_aug_train_ds = aug_train_ds.skip(300)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wcgl7-QwhqR"
      },
      "source": [
        "# Setting up the Bert Model again\n",
        "aug_bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
        "aug_tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "aug_tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZA875tFw0Jb"
      },
      "source": [
        "aug_bert_preprocess_model = hub.KerasLayer(aug_tfhub_handle_preprocess)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOZ3xan6w-kq"
      },
      "source": [
        "aug_bert_model = hub.KerasLayer(aug_tfhub_handle_encoder)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_2jYefxEhF"
      },
      "source": [
        "def build_aug_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(aug_tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(aug_tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qesvt0h9yo--"
      },
      "source": [
        "#Setting up the loss function\n",
        "aug_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "aug_metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkT1oO4LxYpA"
      },
      "source": [
        "# Setting up the epochs and optimizer (AdamW optimizer) \n",
        "\n",
        "steps_per_epoch = tf.data.experimental.cardinality(aug_train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maLf46yIxmzt"
      },
      "source": [
        "#Compiling the augmented classifier model\n",
        "aug_classifier_model = build_aug_classifier_model()\n",
        "aug_classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=aug_loss,\n",
        "                         metrics=aug_metrics)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcTFJ5FyA1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26543551-3d72-4c2c-c2ad-87bb5423cae5"
      },
      "source": [
        "#Training the model to the augmented dataset \n",
        "history = aug_classifier_model.fit(x=real_aug_train_ds,\n",
        "                               validation_data=aug_val_ds,\n",
        "                               epochs= epochs)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1263/1263 [==============================] - 572s 447ms/step - loss: 0.4610 - binary_accuracy: 0.7606 - val_loss: 0.2852 - val_binary_accuracy: 0.8720\n",
            "Epoch 2/5\n",
            "1263/1263 [==============================] - 565s 447ms/step - loss: 0.2780 - binary_accuracy: 0.8817 - val_loss: 0.2156 - val_binary_accuracy: 0.9116\n",
            "Epoch 3/5\n",
            "1263/1263 [==============================] - 564s 446ms/step - loss: 0.1871 - binary_accuracy: 0.9272 - val_loss: 0.2116 - val_binary_accuracy: 0.9259\n",
            "Epoch 4/5\n",
            "1263/1263 [==============================] - 562s 445ms/step - loss: 0.1297 - binary_accuracy: 0.9537 - val_loss: 0.1647 - val_binary_accuracy: 0.9553\n",
            "Epoch 5/5\n",
            "1263/1263 [==============================] - 562s 445ms/step - loss: 0.0924 - binary_accuracy: 0.9702 - val_loss: 0.1367 - val_binary_accuracy: 0.9677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs09mQCqyMno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a02a89-7934-474b-c783-405b92d3844e"
      },
      "source": [
        "aug_loss, aug_accuracy = aug_classifier_model.evaluate(test_ds)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 140s 179ms/step - loss: 0.7237 - binary_accuracy: 0.8646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVbhJLyJFgFI",
        "outputId": "316dbd0d-0dbc-49ee-e7ac-60e317f1b278"
      },
      "source": [
        "print(f'Aug Loss: {aug_loss}')\n",
        "print(f'Aug Accuracy: {aug_accuracy}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aug Loss: 0.7237421274185181\n",
            "Aug Accuracy: 0.8645600080490112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy48S-jH3ZPp"
      },
      "source": [
        "So there is a slight improvement in accuracy, but not significant enough. This could be due to the fact that we only used one augmentation technique (swap augmentation) and these movie reviews are fairly long.  So swapping a pair of words in a long movie review might not change the data sufficiently enough. Also the dataset is already very large at 25000 items so augmenting it might not help much, espcecially if our augmentation is subtle. "
      ]
    }
  ]
}